# Modality-Preference

<div align="center">

# <img src="left-right_brain.png" height="28px"> Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation



<div>
    <a href='http://arxiv.org/abs/2505.20897' target='_blank'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
<!--     <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a> -->
</div>

</div>
This repository contains the official implementation of our paper, Evaluating and Steering Modality Preferences in Multimodal Large Language Model.

We provide the data in $MC^2$ for evaluating modality preference and controlling modality preference through noisy images or text context with grammer errors in this repository.

The complete data of $MC^2$ can be found in [**ðŸ¤—Huggingface**](https://huggingface.co/271754echo/MC2)


